import numpy as np
import tensorflow as tf
from dataclasses import dataclass
from typing import Union

sigmoid = lambda x: 1 / (1 + np.exp(-x))
inv_sigmoid = lambda x: np.log(x / (1 - x))

@dataclass
class RecommenderMockDataset:
    users: np.ndarray
    items: np.ndarray
    items_pop_idx: np.ndarray
    logits: np.ndarray
    clicks: np.ndarray
    items_pop: np.ndarray
    # model parameters
    beta_user: np.ndarray = None
    beta_item: np.ndarray = None
    pop_bias: np.ndarray = None
    intercept: float = None
    pop_beta: float = None
    pop_bias_mu: float = None
    pop_bias_scale: float = None


def generate_map_recommend_mockdata(
    n_features: int=5,
    n_users: int=10000,
    n_items: int=500,
    n_pops: int=100,
    p: float=0.05,
    pop_beta: float=0.5,
    seed: int=None,
    pop_eps: float=1e-6,
    unbias: bool=False,
    **kwargs
):
    """
    generate MAP(Maximum A Posteriori Estimation) recommender mock data,\
    this dataset aims to get the point estimate of the popularity bias of items.
    
    The data is generated by the following formula:\n
    `logits = users @ beta_user + items @ beta_item + intercept + item_pop_bias + noise`\n
    in which, `item_pop_bias` sampled from exponential distribution with rate `pop_beta`,\n
    """
    def _generate_clicks(gen, beta_user, beta_item, intercept, pop_bias):
        # generate user and item features
        sick = kwargs.get('sick', None)
        if sick is not None:
            rho, n_corr = sick['rho'], sick['n_corr']
            users = generate_highly_correlated_data(gen, n_users, n_features, rho, n_corr)
            items = generate_highly_correlated_data(gen, n_items, n_features, rho, n_corr)
        else:
            cov_users = np.eye(n_features) / n_features**2
            users = gen.multivariate_normal(mean=np.zeros(n_features), cov=cov_users, size=n_users)
            # generate items features
            mean_items = np.zeros(n_features)
            cov_items = np.eye(n_features) / n_features**2
            items = gen.multivariate_normal(mean=mean_items, cov=cov_items, size=n_items)
        # generate items pop clusters
        items_pop_idx = np.arange(n_items) % n_pops
        gen.shuffle(items_pop_idx)
        
        logits = ((users @ beta_user)[:, np.newaxis] + (items @ beta_item)[np.newaxis, :]) + intercept
        if not unbias:
            # get pop_bias for each item
            pop_bias_ = pop_bias[items_pop_idx]
            logits = logits + pop_bias_[np.newaxis, :]
        
        # generate binary data
        probs = sigmoid(logits)
        clicks = gen.binomial(n=1, p=probs)

        return users, items, logits, clicks, items_pop_idx
    
    # get random generator
    seed = seed or np.random.randint(0, int(1e6))
    gen = np.random.RandomState(seed)

    # generate beta
    if kwargs.get('params') is None:
        beta_user = gen.normal(loc=0, scale=1, size=n_features) 
        beta_item = gen.normal(loc=0, scale=1, size=n_features)
        intercept = inv_sigmoid(p)
        # generate popularity
        pop_bias = gen.exponential(scale=pop_beta, size=(n_pops, ))
    else:
        beta_user = kwargs['params']['beta_user']
        beta_item = kwargs['params']['beta_item']
        intercept = kwargs['params']['intercept']
        pop_bias  = kwargs['params']['pop_bias']

    # generate clicks data
    users, items, logits, clicks, items_pop_idx = _generate_clicks(gen, beta_user, beta_item, intercept, pop_bias)

    # calculate items popularity
    items_pop = clicks.sum(axis=0)
    items_pop = items_pop / items_pop.sum()
    items_pop = (items_pop - items_pop.min()) / (items_pop.max() - items_pop.min())
    items_pop[items_pop < pop_eps] = pop_eps
    
    dataset = RecommenderMockDataset(
        users, items, items_pop_idx, logits, clicks, items_pop,
        beta_user, beta_item, pop_bias, intercept, pop_beta
    )
    return dataset


def generate_bayesian_recommend_mockdata(
    n_features: int=5,
    n_users: int=10000,
    n_items: int=1000,
    n_pops: int=200,
    p: float=0.05,
    pop_beta: float=0.5,
    seed: int=None,
    pop_eps: float=1e-6,
    unbias: bool=False,
    pop_dist: str="lognormal",
    pop_bias_scale: Union[float, np.ndarray]=0.1,
    **kwargs
):
    """
    generate Bayesian recommender mock data,\
    this dataset aims to get the posterior distribution of the popularity bias of items.

    The data is generated by the following herarchical bayesian model:\n
    1. sample pop_bias `mu` from a exponential distribution with rate `pop_beta`.
    2. for each item, sample popularity bias `pop_bias` from a distribution F with `mu` as mean.\n
    `logits = users @ beta_user + items @ beta_item + intercept + item_pop_bias + noise`\n
    in which, `item_pop_bias` sampled from a distribution `F(mu, ...)`, F can be normal, log-normal, etc

    """
    def _generate_clicks(gen, beta_user, beta_item, intercept, pop_bias_mu, pop_bias_scale):
        # generate user and item features
        cov_users = np.eye(n_features) / n_features**2
        users = gen.multivariate_normal(mean=np.zeros(n_features), cov=cov_users, size=n_users)
        # generate items features
        mean_items = np.zeros(n_features)
        cov_items = np.eye(n_features) / n_features**2
        items = gen.multivariate_normal(mean=mean_items, cov=cov_items, size=n_items)
        # generate items pop clusters
        items_pop_idx = np.arange(n_items) % n_pops
        gen.shuffle(items_pop_idx)
        
        logits = ((users @ beta_user)[:, np.newaxis] + (items @ beta_item)[np.newaxis, :]) + intercept
        if not unbias:
            # sample popularity bias for each item
            for i in range(n_items):
                mu = pop_bias_mu[items_pop_idx[i]]
                scale = pop_bias_scale[items_pop_idx[i]]
                if pop_dist == 'normal':
                    pop_bias = gen.normal(loc=mu, scale=scale, size=n_users)
                elif pop_dist == 'lognormal':
                    mu = np.log(mu)
                    pop_bias = gen.lognormal(mu, scale, size=n_users)
                else:
                    raise ValueError(f"Invalid popularity distribution: {pop_dist}")
                logits[:, i] += pop_bias
        
        # generate binary data
        probs = sigmoid(logits)
        clicks = gen.binomial(n=1, p=probs)

        return users, items, logits, clicks, items_pop_idx
    
    # get random generator
    seed = seed or np.random.randint(0, int(1e6))
    gen = np.random.RandomState(seed)

    # generate beta
    if kwargs.get('params') is None:
        beta_user = gen.normal(loc=0, scale=1, size=n_features) 
        beta_item = gen.normal(loc=0, scale=1, size=n_features)
        intercept = inv_sigmoid(p)
        # generate popularity
        pop_bias_mu = gen.exponential(scale=pop_beta, size=(n_pops, ))
    else:
        beta_user   = kwargs['params']['beta_user']
        beta_item   = kwargs['params']['beta_item']
        intercept   = kwargs['params']['intercept']
        pop_bias_mu = kwargs['params']['pop_bias_mu']
    
    if isinstance(pop_bias_scale, (int, float)):
        pop_bias_scale = [pop_bias_scale] * n_pops
    else:
        assert len(pop_bias_scale) == n_pops, "pop_bias_scale must have the same length as n_pops"
    # generate user and item features
    users, items, logits, clicks, items_pop_idx = \
        _generate_clicks(gen, beta_user, beta_item, intercept, pop_bias_mu, pop_bias_scale)
    
    # calculate items popularity
    items_pop = clicks.sum(axis=0)
    items_pop = items_pop / items_pop.sum()
    items_pop = (items_pop - items_pop.min()) / (items_pop.max() - items_pop.min())
    items_pop[items_pop < pop_eps] = pop_eps
    
    return RecommenderMockDataset(
        users, items, items_pop_idx, logits, clicks, items_pop,
        beta_user, beta_item, None, intercept, pop_beta, pop_bias_mu, pop_bias_scale
    )


def create_dataloader(
    data: RecommenderMockDataset,
    batch_size: int=1024,
    shuffle: bool=True
):
    def _batch_generator():
        n_users, n_items = data.clicks.shape
        n_batches = (n_users * n_items) // batch_size
        indices = np.arange(n_users * n_items)
        if shuffle:
            np.random.shuffle(indices)
        for i in range(n_batches):
            # get batch indices
            batch_indices = indices[i * batch_size: (i + 1) * batch_size]
            
            # get user and item indices
            user_indices = batch_indices // n_items
            item_indices = batch_indices % n_items
            # get user and item features
            users = data.users[user_indices]
            items = data.items[item_indices]
            # get clicks
            clicks = data.clicks[user_indices, item_indices]
            # get pop idx
            items_pop_idx = data.items_pop_idx[item_indices]

            yield user_indices, item_indices, items_pop_idx, users, items, clicks

    return tf.data.Dataset.from_generator(
        _batch_generator, 
        output_signature=(
            tf.TensorSpec(shape=(batch_size, ), dtype=tf.int32),
            tf.TensorSpec(shape=(batch_size, ), dtype=tf.int32),
            tf.TensorSpec(shape=(batch_size, ), dtype=tf.int32),
            tf.TensorSpec(shape=(batch_size, data.users.shape[1]), dtype=tf.float32),
            tf.TensorSpec(shape=(batch_size, data.items.shape[1]), dtype=tf.float32),
            tf.TensorSpec(shape=(batch_size, ), dtype=tf.float32)
        )
    )


def generate_highly_correlated_data(
    gen: np.random.RandomState,
    n_samples: int,
    n_features: int,
    rho: float=0.9,
    n_corr: int=0
):
    # generate corr matrix
    corr = rho / n_features**2
    Sigma = np.full((n_features, n_features), 0.0)
    # random correlation
    redundent_features = gen.choice(n_features, n_corr+1, replace=False)
    for i in range(n_features):
        for j in range(i, n_features):
            if i == j:
                Sigma[i, j] = 1 / n_features**2
            else:
                if i in redundent_features and j in redundent_features:
                    Sigma[i, j] = corr
                    Sigma[j, i] = corr
    
    mean = np.zeros(n_features)
    X = gen.multivariate_normal(mean, Sigma, size=n_samples)
    return X